# 신경망 개요
퍼셉트론 알고리즘을 활용하면 이론상 모든 컴퓨터의 기능을 구현할 수 있지만, 이를 위한 가중치를 개발자가 일일히 지정해줘야 하는 큰 단점이 있었다.
<p>신경망에서는, 이 단점을 보완하기 위해 컴퓨터가 데이터로부터 자동으로 이러한 가중치를 구한다.</p>

신경망은 입력층, 은닉층, 출력층으로 이루어지는데, 개발자는 이 중 입력층과 출력층만을 보고 조작할 수 있다.

## 활성화 함수
활성화 함수(Activation Function)은 입력 신호의 총합이 활성화를 일으키는지 정하는 역할을 한다. 이 활성화 함수는 전에 배운 퍼셉트론의 식과 유사하게 아래처럼 나타낼 수 있다.

`h(x) = b + w1x1 + w2x2`

이러한 활성화 함수는 임계값을 경계로 출력이 바뀌는데, 이를 계단 함수(step function)이라 한다.

위 함수에서는 활성화 함수를 계단 함수로 사용했지만, 활성화 함수를 다른 함수로 구현할 수도 있는데, 이것이 신경망의 기본 원리이다.

### 시그모이드 함수
다음은 신경망에서 자주 사용하는 활성화 함수인 시그모이드 함수(Sigmoid Function)이다.

![img](./img/sigmoid.png)

### 계단 함수와 시그모이드 함수 비교
![img](./img/stepFunction.png)
![img](./img/sigmoidFunction.png)

특정값에서 급작스럽게 값이 변하는 계단 함수와 달리, 시그모이드 함수는 그 이름대로 S자의 함수를 형성한다.

또한, 계단 함수가 0과 1만을 출력하는 것에 비해, 시그모이드 함수는 0에서 1사이의 연속되는 실수를 리턴값으로 가진다.

퍼셉트론에서 0,1이 흘렀다면, 신경망에서는 연속적인 실수가 흐르는 것이다.

### 비선형 함수
함수를 논할 때 선형 함수와 비선형 함수라는 용어가 등장한다.
선형 함수는 `y=ax + b`와 같이 기울기가 일정한 함수이며, 비선형 함수는 기울기가 일정하지 않은 함수이다.

신경망에서 활성화 함수는 비선형 함수를 써야한다. 그렇지 않으면 신경망의 계층을 깊게 하는 의미가 없어지기 때문이다.

선형 함수로 구성된 3층 퍼셉트론을 생각해보자.
`h(x) = cx`를 활성화 함수로 한다고 했을때, 결과는 `y=h(h(h(x)))`가 될 것이다.
이는 결국 a가 c의 3승인 `y=ax`와 같다.

여러 계층의 이점을 얻으려면, 반드시 비선형 함수를 활성화 함수로 써야하는 것이다.

### ReLU 함수
시그모이드 함수외에도 최근 신경망에 자주 사용되는 함수로 ReLU 함수(Rectified Linier Unit)가 있다.

ReLU함수는 0이하의 값은 0을 리턴하고, 이외의 값은 그대로 리턴하는 함수로 아래와 같다.
![img](./img/ReLUFunction.png)

## 넘파이의 다차원 배열
다차원 배열 계산을 숙달하면 신경망을 효율적으로 개발할 수 있다.

이와 관련해, 넘파이의 다차원 행렬 계산 코드를 matrix with numpy 디렉토리에 저장해두었다.

## 신경망에서의 행렬 곱
다음 신경망을 행렬로 구현해보자.

![img](./img/matrixnet.png)

X는 2, W는 2x3, Y는 3으로 행렬곱이 가능하다.

~~~python
import numpy as np
X = np.array([1,2])
W = np.array([[1,3,5],[2,4,6]])

Y = np.dot(X,W)
~~~

다차원 배열의 스칼라곱을 구하는 np.dot을 활용하면 Y의 원소가 몇개든 쉽게 계산할 수 있다.

## 실습: 3층 신경망 구현하기
이제 넘파이를 통해 3층 신경망을 구현해보자.
이는 내용이 길어져 Chapter 3 안의 Multi-Layer_Neural_Network로 분류했다.